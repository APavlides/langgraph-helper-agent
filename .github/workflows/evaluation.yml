name: RAGAS Evaluation

on:
  schedule:
    # Run every Monday at 6 AM UTC
    - cron: "0 6 * * 1"
  workflow_dispatch:
    inputs:
      mode:
        description: "Evaluation mode"
        required: true
        default: "offline"
        type: choice
        options:
          - offline
          - online

env:
  PYTHON_VERSION: "3.11"

jobs:
  evaluate:
    name: RAGAS Evaluation (${{ github.event.inputs.mode || 'offline' }} mode)
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Setup Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          sleep 5
          ollama pull llama3.2:3b
          ollama pull nomic-embed-text
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[dev]"
      
      - name: Download documentation
        run: python scripts/refresh_data.py --full
      
      - name: Build vector store
        run: python scripts/build_vectorstore.py
      
      - name: Run RAGAS evaluation
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          AGENT_MODE: ${{ github.event.inputs.mode || 'offline' }}
        run: |
          python -m evaluation.evaluate \
            --mode ${{ github.event.inputs.mode || 'offline' }} \
            --output evaluation/reports/report-${{ github.run_id }}.json
      
      - name: Generate markdown report
        run: |
          python -c "
          import json
          from pathlib import Path
          
          report_path = Path('evaluation/reports/report-${{ matrix.mode }}-${{ github.run_id }}.json')
          report = json.loads(report_path.read_text())
          
          md = f'''# Evaluation Report - ${{ matrix.mode }} mode
          
          **Run ID:** ${{ github.run_id }}
          **Timestamp:** {report['metadata']['timestamp']}
          
          ## Summary
          
          | Metric | Score |
          |--------|-------|
          | Success Rate | {report['aggregate_metrics']['success_rate']:.1%} |
          | Avg Aggregate Score | {report['aggregate_metrics']['avg_aggregate_score']:.3f} |
          | Avg Context Relevancy | {report['aggregate_metrics']['avg_context_relevancy']:.3f} |
          | Avg Faithfulness | {report['aggregate_metrics']['avg_faithfulness']:.3f} |
          | Avg Answer Relevancy | {report['aggregate_metrics']['avg_answer_relevancy']:.3f} |
          | Avg Topic Coverage | {report['aggregate_metrics']['avg_topic_coverage']:.3f} |
          | Avg Latency (ms) | {report['aggregate_metrics']['avg_latency_ms']:.0f} |
          
          ## Scores by Category
          
          | Category | Score |
          |----------|-------|
          '''
          
          for cat, score in report['scores_by_category'].items():
              md += f'| {cat} | {score:.3f} |\\n'
          
          md += '''
          ## Scores by Difficulty
          
          | Difficulty | Score |
          |------------|-------|
          '''
          
          for diff, score in report['scores_by_difficulty'].items():
              md += f'| {diff} | {score:.3f} |\\n'
          
          Path('evaluation/reports/report-${{ matrix.mode }}-${{ github.run_id }}.md').write_text(md)
          "
      
      - name: Upload evaluation report
        uses: actions/upload-artifact@v4
        with:
          name: eval-report-${{ matrix.mode }}-${{ github.run_id }}
          path: |
            evaluation/reports/report-${{ matrix.mode }}-${{ github.run_id }}.json
            evaluation/reports/report-${{ matrix.mode }}-${{ github.run_id }}.md
          retention-days: 90
      
      - name: Post summary
        run: |
          echo "## Evaluation Results (${{ matrix.mode }} mode)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat evaluation/reports/report-${{ matrix.mode }}-${{ github.run_id }}.md >> $GITHUB_STEP_SUMMARY

  compare:
    name: Compare Results
    runs-on: ubuntu-latest
    needs: evaluate
    if: ${{ github.event.inputs.mode == 'both' || github.event.inputs.mode == '' }}
    steps:
      - name: Download offline report
        uses: actions/download-artifact@v4
        with:
          name: eval-report-offline-${{ github.run_id }}
          path: reports/offline
      
      - name: Download online report
        uses: actions/download-artifact@v4
        with:
          name: eval-report-online-${{ github.run_id }}
          path: reports/online
      
      - name: Compare modes
        run: |
          echo "## Mode Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          python3 -c "
          import json
          from pathlib import Path
          
          offline = json.loads(list(Path('reports/offline').glob('*.json'))[0].read_text())
          online = json.loads(list(Path('reports/online').glob('*.json'))[0].read_text())
          
          print('| Metric | Offline | Online | Î” |')
          print('|--------|---------|--------|---|')
          
          metrics = ['avg_aggregate_score', 'avg_context_relevancy', 'avg_faithfulness', 'avg_answer_relevancy', 'avg_topic_coverage']
          
          for m in metrics:
              off_val = offline['aggregate_metrics'][m]
              on_val = online['aggregate_metrics'][m]
              delta = on_val - off_val
              sign = '+' if delta > 0 else ''
              print(f'| {m.replace(\"avg_\", \"\").replace(\"_\", \" \").title()} | {off_val:.3f} | {on_val:.3f} | {sign}{delta:.3f} |')
          " >> $GITHUB_STEP_SUMMARY
